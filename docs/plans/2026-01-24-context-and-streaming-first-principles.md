# 第一性原理分析：章节生成的上下文收缩与流式输出

日期：2026-01-24

## Phase 1：问题定义

当前章节生成（`app/api/novels/[id]/generate/route.ts`）在小说变长后会出现两个“会把产品从 MVP 拖垮到不可用”的核心风险：

1. **上下文膨胀（Token 溢出 + 注意力分散）**
   - 生成前会把“全部人物卡 + 全部世界观 + 全部未揭示伏笔”注入 prompt。
   - 同时会 `SELECT *` 拉取该小说全部章节（包含完整 `content`），仅用于计算下一章编号和拼接前情提要。
2. **长文本生成体验差 / 容易超时**
   - API 为阻塞式生成，前端只能转圈等待；生成 3000 字在网络抖动或 serverless 环境下更容易超时。

成功标准（可验证）：
- 小说人物/设定规模变大时，**prompt 注入规模随“相关性”而非“总量”增长**。
- 生成过程对用户可见（可流式看到输出），并且章节最终仍能落库、跳转阅读。

## Phase 2：显式化当前假设

当前实现隐含假设：
- A1：为了“一致性”，必须把所有人物/设定都塞进 prompt。
- A2：生成必须一次性完成后再返回，流式并非必要。
- A3：读取章节时，直接拉全量章节内容不会成为瓶颈。

## Phase 3：逐条质疑假设

- A1（**惯例**）：一致性需要“相关信息充分”而非“信息全量”。全量注入反而带来注意力分散与成本飙升。
- A2（**惯例**）：生成的本质是长耗时 IO；“先反馈、后完成”是更合理的人机交互。
- A3（**惯例**）：章节内容是最大的字段，随着章节数增长会线性拖慢生成接口，属于可预见的扩展性问题。

## Phase 4：不可约束（Fundamentals）

无论用什么模型/框架，都必须满足：
- F1：LLM 上下文窗口有限（硬约束）；需要“预算管理”。
- F2：一致性依赖“关键记忆命中率”，而关键记忆必须可被检索到且可控地注入。
- F3：章节生成属于长耗时任务；用户需要过程反馈，系统需要避免超时。

## Phase 5：从零重建（推荐方案）

### 方案（推荐）：生成前做“上下文收缩”，生成时做“流式输出”

1. **上下文收缩（Context Builder）**
   - 章节编号用 `COUNT(*)` 获取；前情提要只取最近 N 章。
   - 人物/世界观不再全量注入：使用“名称/标题命中 + 最近出现章节”进行简单相关性排序，取 Top-K。
   - 伏笔分三类注入：
     - 本章需揭示（目标）
     - 需要开始铺垫（引导）
     - 长期未回收（背景约束，少量 Top-K）

2. **流式输出（Streaming）**
   - API 支持流式返回文本增量（同时在服务端累计内容，结束后落库并返回 `chapterId`）。
   - 前端边接收边展示，让用户看到“正在生成”，同时降低超时体感。

### 替代方案（后续增强）
- 用 embedding（pgvector）对人物/设定/伏笔做语义检索替代“名称命中”。
- 将 LightRAG 的索引构建改为异步任务（队列/任务表），避免请求内阻塞。

## Phase 6：验证与风险

验证方式（按项目规范）：
- 本地启动 `npm run dev`，用 `agent-browser` 完整跑通：注册/登录 → 创建小说 → 生成章节（流式可见）→ 自动落库 → 跳转阅读。

主要风险与缓解：
- 相关性检索的启发式可能漏掉关键设定：通过“最近出现”兜底 + 可调 Top-K 限额缓解。
- 不同 AI provider 的流式协议不同：先实现 OpenAI/Anthropic（DeepSeek 属于 OpenAI 兼容），Gemini 暂回退到非流式（仍能跑通）。

